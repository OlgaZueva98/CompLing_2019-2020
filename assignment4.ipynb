{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "assignment4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-Lre1X2l4R1",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 4: Named entity recognition\n",
        "\n",
        "Create a model for Named Entity Recognition for dataset CoNLL 2002.  \n",
        "Your quality metric = f1_macro\n",
        "\n",
        "In your solution you should use: RandomForest, Gradient Boosting (xgboost, lightgbm, catboost)   \n",
        "Tutorials:  \n",
        "1. https://github.com/Microsoft/LightGBM/tree/master/examples/python-guide\n",
        "1. https://github.com/catboost/tutorials \n",
        "\n",
        "More baselines you beat - better your score\n",
        " \n",
        "baseline 1 [3 points]: 0.0604      random labels  \n",
        "baseline 2 [5 points]: 0.3966      PoS features + logistic regression  \n",
        "baseline 3 [8 points]: 0.8122      word2vec cbow embedding + baseline 2 + svm    \n",
        "\n",
        "[1 point] using feature engineering (creating features not presented in the baselines)\n",
        "\n",
        "! Your results must be reproducible. You should explicitly set all seeds random_states in yout model.  \n",
        "! Remember to use proper training pipeline.  \n",
        "\n",
        "bonus, think about:  \n",
        "1. [1 point] Why did we select f1 score with macro averaging as our classification quality measure? What other metrics are suitable?   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gzVubUumvUv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "import io"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZb7cMLpoT9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zf = zipfile.ZipFile('data.zip', \"r\")\n",
        "zf.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5Dr7Pphl4R4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn import model_selection\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn import metrics\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "SEED=1337"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6FJZQM2l4R8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "0cf064d8-17d8-49aa-b864-5e7c66ff1451"
      },
      "source": [
        "df = pd.read_csv('data/ner_short.csv', index_col=0)\n",
        "df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>next-next-pos</th>\n",
              "      <th>next-next-word</th>\n",
              "      <th>next-pos</th>\n",
              "      <th>next-word</th>\n",
              "      <th>pos</th>\n",
              "      <th>prev-pos</th>\n",
              "      <th>prev-prev-pos</th>\n",
              "      <th>prev-prev-word</th>\n",
              "      <th>prev-word</th>\n",
              "      <th>sentence_idx</th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NNS</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>IN</td>\n",
              "      <td>of</td>\n",
              "      <td>NNS</td>\n",
              "      <td>__START1__</td>\n",
              "      <td>__START2__</td>\n",
              "      <td>__START2__</td>\n",
              "      <td>__START1__</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>VBP</td>\n",
              "      <td>have</td>\n",
              "      <td>NNS</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>IN</td>\n",
              "      <td>NNS</td>\n",
              "      <td>__START1__</td>\n",
              "      <td>__START1__</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>1.0</td>\n",
              "      <td>of</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>VBN</td>\n",
              "      <td>marched</td>\n",
              "      <td>VBP</td>\n",
              "      <td>have</td>\n",
              "      <td>NNS</td>\n",
              "      <td>IN</td>\n",
              "      <td>NNS</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>of</td>\n",
              "      <td>1.0</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>IN</td>\n",
              "      <td>through</td>\n",
              "      <td>VBN</td>\n",
              "      <td>marched</td>\n",
              "      <td>VBP</td>\n",
              "      <td>NNS</td>\n",
              "      <td>IN</td>\n",
              "      <td>of</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>1.0</td>\n",
              "      <td>have</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NNP</td>\n",
              "      <td>London</td>\n",
              "      <td>IN</td>\n",
              "      <td>through</td>\n",
              "      <td>VBN</td>\n",
              "      <td>VBP</td>\n",
              "      <td>NNS</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>have</td>\n",
              "      <td>1.0</td>\n",
              "      <td>marched</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  next-next-pos next-next-word next-pos  ... sentence_idx           word tag\n",
              "0           NNS  demonstrators       IN  ...          1.0      Thousands   O\n",
              "1           VBP           have      NNS  ...          1.0             of   O\n",
              "2           VBN        marched      VBP  ...          1.0  demonstrators   O\n",
              "3            IN        through      VBN  ...          1.0           have   O\n",
              "4           NNP         London       IN  ...          1.0        marched   O\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-theVxEGl4SB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "13f00de2-971c-48de-cfc4-31dfa0e4a6b9"
      },
      "source": [
        "# number of sentences\n",
        "df.sentence_idx.max()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1500.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFnMAH54l4SE",
        "colab_type": "code",
        "colab": {},
        "outputId": "5771cc56-c2b3-4f8d-fc9a-1e62cf26ecbc"
      },
      "source": [
        "# class distribution\n",
        "df.tag.value_counts(normalize=True )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "O        0.852828\n",
              "B-geo    0.027604\n",
              "B-gpe    0.020935\n",
              "B-org    0.020247\n",
              "I-per    0.017795\n",
              "B-tim    0.016927\n",
              "B-per    0.015312\n",
              "I-org    0.013937\n",
              "I-geo    0.005383\n",
              "I-tim    0.004247\n",
              "B-art    0.001376\n",
              "I-gpe    0.000837\n",
              "I-art    0.000748\n",
              "B-eve    0.000628\n",
              "I-eve    0.000508\n",
              "B-nat    0.000449\n",
              "I-nat    0.000239\n",
              "Name: tag, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS-OUELIl4SH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sentence length\n",
        "tdf = df.set_index('sentence_idx')\n",
        "tdf['length'] = df.groupby('sentence_idx').tag.count()\n",
        "df = tdf.reset_index(drop=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjeii3Kfl4SK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encode categorial variables\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['pos'] = le.fit_transform(df.pos)\n",
        "df['next-pos'] = le.fit_transform(df['next-pos'])\n",
        "df['next-next-pos'] = le.fit_transform(df['next-next-pos'])\n",
        "df['prev-pos'] = le.fit_transform(df['prev-pos'])\n",
        "df['prev-prev-pos'] = le.fit_transform(df['prev-prev-pos'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCGj1eH-l4SM",
        "colab_type": "code",
        "colab": {},
        "outputId": "db01aa5c-c42a-4b3d-d6a4-7fce0a61781c"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_idx</th>\n",
              "      <th>next-next-pos</th>\n",
              "      <th>next-next-word</th>\n",
              "      <th>next-pos</th>\n",
              "      <th>next-word</th>\n",
              "      <th>pos</th>\n",
              "      <th>prev-pos</th>\n",
              "      <th>prev-prev-pos</th>\n",
              "      <th>prev-prev-word</th>\n",
              "      <th>prev-word</th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>18</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>9</td>\n",
              "      <td>of</td>\n",
              "      <td>18</td>\n",
              "      <td>39</td>\n",
              "      <td>40</td>\n",
              "      <td>__START2__</td>\n",
              "      <td>__START1__</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>O</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>33</td>\n",
              "      <td>have</td>\n",
              "      <td>18</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>9</td>\n",
              "      <td>18</td>\n",
              "      <td>39</td>\n",
              "      <td>__START1__</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>of</td>\n",
              "      <td>O</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>32</td>\n",
              "      <td>marched</td>\n",
              "      <td>33</td>\n",
              "      <td>have</td>\n",
              "      <td>18</td>\n",
              "      <td>9</td>\n",
              "      <td>18</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>of</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>O</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>through</td>\n",
              "      <td>32</td>\n",
              "      <td>marched</td>\n",
              "      <td>33</td>\n",
              "      <td>18</td>\n",
              "      <td>9</td>\n",
              "      <td>of</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>have</td>\n",
              "      <td>O</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>16</td>\n",
              "      <td>London</td>\n",
              "      <td>9</td>\n",
              "      <td>through</td>\n",
              "      <td>32</td>\n",
              "      <td>33</td>\n",
              "      <td>18</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>have</td>\n",
              "      <td>marched</td>\n",
              "      <td>O</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentence_idx  next-next-pos next-next-word  next-pos      next-word  pos  \\\n",
              "0           1.0             18  demonstrators         9             of   18   \n",
              "1           1.0             33           have        18  demonstrators    9   \n",
              "2           1.0             32        marched        33           have   18   \n",
              "3           1.0              9        through        32        marched   33   \n",
              "4           1.0             16         London         9        through   32   \n",
              "\n",
              "   prev-pos  prev-prev-pos prev-prev-word      prev-word           word tag  \\\n",
              "0        39             40     __START2__     __START1__      Thousands   O   \n",
              "1        18             39     __START1__      Thousands             of   O   \n",
              "2         9             18      Thousands             of  demonstrators   O   \n",
              "3        18              9             of  demonstrators           have   O   \n",
              "4        33             18  demonstrators           have        marched   O   \n",
              "\n",
              "   length  \n",
              "0      48  \n",
              "1      48  \n",
              "2      48  \n",
              "3      48  \n",
              "4      48  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SZI6W73l4SQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6b93052a-7f3b-4d80-bf3e-6968642ced73"
      },
      "source": [
        "# splitting\n",
        "y = LabelEncoder().fit_transform(df.tag)\n",
        "\n",
        "df_train, df_test, y_train, y_test = model_selection.train_test_split(df, y, stratify=y, \n",
        "                                                                      test_size=0.25, random_state=SEED, shuffle=True)\n",
        "print('train', df_train.shape[0])\n",
        "print('test', df_test.shape[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train 50155\n",
            "test 16719\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iDEN-Cwl4ST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# some wrappers to work with word2vec\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.base import TransformerMixin\n",
        "from collections import defaultdict\n",
        "\n",
        "   \n",
        "class Word2VecWrapper(TransformerMixin):\n",
        "    def __init__(self, window=5,negative=5, size=100, iter=100, is_cbow=False, random_state=SEED):\n",
        "        self.window_ = window\n",
        "        self.negative_ = negative\n",
        "        self.size_ = size\n",
        "        self.iter_ = iter\n",
        "        self.is_cbow_ = is_cbow\n",
        "        self.w2v = None\n",
        "        self.random_state = random_state\n",
        "        \n",
        "    def get_size(self):\n",
        "        return self.size_\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        X: list of strings\n",
        "        \"\"\"\n",
        "        sentences_list = [x.split() for x in X]\n",
        "        self.w2v = Word2Vec(sentences_list, \n",
        "                            window=self.window_,\n",
        "                            negative=self.negative_, \n",
        "                            size=self.size_, \n",
        "                            iter=self.iter_,\n",
        "                            sg=not self.is_cbow_, seed=self.random_state)\n",
        "\n",
        "        return self\n",
        "    \n",
        "    def has(self, word):\n",
        "        return word in self.w2v\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        X: a list of words\n",
        "        \"\"\"\n",
        "        if self.w2v is None:\n",
        "            raise Exception('model not fitted')\n",
        "        return np.array([self.w2v[w] if w in self.w2v else np.zeros(self.size_) for w in X ])\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C10dXA-Wl4SW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cea600f3-8a7d-459f-80a3-b79043659199"
      },
      "source": [
        "%%time\n",
        "# here we exploit that word2vec is an unsupervised learning algorithm\n",
        "# so we can train it on the whole dataset (subject to discussion)\n",
        "\n",
        "sentences_list = [x.strip() for x in ' '.join(df.word).split('.')]\n",
        "\n",
        "w2v_cbow = Word2VecWrapper(window=5, negative=5, size=300, iter=300, is_cbow=True, random_state=SEED)\n",
        "w2v_cbow.fit(sentences_list)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 48.8 s, sys: 461 ms, total: 49.2 s\n",
            "Wall time: 27.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_ltmBoUl4SZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "fe30d015-1129-43cf-f33d-9541ec23ddc4"
      },
      "source": [
        "%%time\n",
        "# baseline 1 \n",
        "# Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "columns = ['pos','next-pos','next-next-pos','prev-pos','prev-prev-pos']\n",
        "\n",
        "model = Pipeline([\n",
        "    ('enc', OneHotEncoder()),\n",
        "    ('est', RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=SEED))\n",
        "])\n",
        "\n",
        "model.fit(df_train[columns], y_train)\n",
        "\n",
        "print('train', metrics.f1_score(y_train, model.predict(df_train[columns]), average='macro'))\n",
        "print('test', metrics.f1_score(y_test, model.predict(df_test[columns]), average='macro'))\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train 0.7478077999134668\n",
            "test 0.5999734506534447\n",
            "CPU times: user 5min 36s, sys: 3.11 s, total: 5min 39s\n",
            "Wall time: 2min 51s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T04WDp-vl4Se",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c8a4b398-8e0d-4719-d7e9-e524540254f5"
      },
      "source": [
        "%%time\n",
        "# baseline 2 \n",
        "# pos features + one hot encoding + XGBoost\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "columns = ['pos','next-pos','next-next-pos','prev-pos','prev-prev-pos']\n",
        "\n",
        "params = {\n",
        "    'learning_rate': [0.05],\n",
        "    'n_estimators': [1000],\n",
        "    'max_depth': [8],\n",
        "    'objective':['binary:logistic']\n",
        "}\n",
        "\n",
        "model = Pipeline([\n",
        "    ('enc', OneHotEncoder()),\n",
        "    ('est', GridSearchCV(xgb.XGBClassifier(), params, n_jobs=5))\n",
        "])\n",
        "\n",
        "model.fit(df_train[columns], y_train)\n",
        "\n",
        "print('train', metrics.f1_score(y_train, model.predict(df_train[columns]), average='macro'))\n",
        "print('test', metrics.f1_score(y_test, model.predict(df_test[columns]), average='macro'))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train 0.7192821183912597\n",
            "test 0.5481715950483709\n",
            "CPU times: user 9min 26s, sys: 2.3 s, total: 9min 28s\n",
            "Wall time: 22min 21s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S-vHkjZl4Sh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f903dd27-a629-4f01-968e-06fef01c8031"
      },
      "source": [
        "%%time\n",
        "# baseline 3\n",
        "# use word2vec cbow embedding + Random Forest\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "import scipy.sparse as sp\n",
        "\n",
        "embeding = w2v_cbow\n",
        "encoder_pos = OneHotEncoder()\n",
        "X_train = sp.hstack([\n",
        "    embeding.transform(df_train.word),\n",
        "    embeding.transform(df_train['next-word']),\n",
        "    embeding.transform(df_train['next-next-word']),\n",
        "    embeding.transform(df_train['prev-word']),\n",
        "    embeding.transform(df_train['prev-prev-word']),\n",
        "    encoder_pos.fit_transform(df_train[['pos','next-pos','next-next-pos','prev-pos','prev-prev-pos']])\n",
        "])\n",
        "X_test = sp.hstack([\n",
        "    embeding.transform(df_test.word),\n",
        "    embeding.transform(df_test['next-word']),\n",
        "    embeding.transform(df_test['next-next-word']),\n",
        "    embeding.transform(df_test['prev-word']),\n",
        "    embeding.transform(df_test['prev-prev-word']),\n",
        "    encoder_pos.transform(df_test[['pos','next-pos','next-next-pos','prev-pos','prev-prev-pos']])\n",
        "])\n",
        "\n",
        "model = RandomForestClassifier(random_state=SEED)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print('train', metrics.f1_score(y_train, model.predict(X_train), average='macro'))\n",
        "print('test', metrics.f1_score(y_test, model.predict(X_test), average='macro'))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train 0.9884874725199649\n",
            "test 0.8299194991539722\n",
            "CPU times: user 1min 13s, sys: 1.19 s, total: 1min 14s\n",
            "Wall time: 1min 14s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6UdEunJl4Sk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}